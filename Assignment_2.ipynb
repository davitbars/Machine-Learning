{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9W8e-SBsgVz"
      },
      "source": [
        "<h2>Assignment 2 - Using a Neural Network to fit the California Housing data</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5DkDQmrsgV2"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q71FjRQDhMdI",
        "outputId": "d18efc50-0ec9-4ad1-ed8f-3c92b5997202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 13)\n"
          ]
        }
      ],
      "source": [
        "# California Housing dataset\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# load data from csv file\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\", \"housing.csv\")\n",
        "housing = pd.read_csv('housing.csv')\n",
        "\n",
        "# Using the setting inplace=False, drop() creates a copy of the data and does not affect housing dataset\n",
        "housing_data = housing.drop(\"median_house_value\", axis=1, inplace=False)\n",
        "housing_target = housing[\"median_house_value\"].copy()\n",
        "feature_names = list(housing_data.columns)\n",
        "\n",
        "#  Transformation pipeline at https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "    ('std_scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    ('num', num_pipeline, feature_names[:-1]),\n",
        "    ('cat', OneHotEncoder(), [feature_names[-1]]),\n",
        "])\n",
        "\n",
        "housing_preprocessed = full_pipeline.fit_transform(housing_data)\n",
        "\n",
        "print(housing_preprocessed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mam-Y-QVsgV5"
      },
      "outputs": [],
      "source": [
        "X = housing_preprocessed\n",
        "y = housing_target.to_numpy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RW370j0sgV5"
      },
      "source": [
        "# (for comparsion) Using scikit-learn's Linear Regression model to fit the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpzf_PWRtXz3",
        "outputId": "cc36f87f-67f8-4ac3-8239-2cc9b56c2e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model training error : 0.647\n",
            "model testing error: 0.640\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# documentation at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "lr = LinearRegression()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "print(\"model training error : %.3f\" % lr.score(X_train, y_train))\n",
        "print(\"model testing error: %.3f\" % lr.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ScwbRUuT8V"
      },
      "source": [
        "# (Task 1 of Assignment 2): Using PyTorch nn.Sequential() to build a neural network to fit the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h29PKvF2sgV6",
        "outputId": "0e53b39c-f6a5-45fd-caf2-08551b6f8fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Loss: 56189616128.0\n",
            "Epoch 50/500, Loss: 56182284288.0\n",
            "Epoch 100/500, Loss: 56134045696.0\n",
            "Epoch 150/500, Loss: 55975866368.0\n",
            "Epoch 200/500, Loss: 55602954240.0\n",
            "Epoch 250/500, Loss: 54877401088.0\n",
            "Epoch 300/500, Loss: 53624619008.0\n",
            "Epoch 350/500, Loss: 51660603392.0\n",
            "Epoch 400/500, Loss: 48791314432.0\n",
            "Epoch 450/500, Loss: 44786122752.0\n",
            "\n",
            "Training error: 39540543488.0\n",
            "Testing error: 39334789120.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "# Number of input features\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Convert the numpy arrays to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).squeeze()\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32).squeeze()\n",
        "\n",
        "# Define the model with deeper architecture and more neurons\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_dim, 256),  # First hidden layer with 256 neurons\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),        # Second hidden layer with 128 neurons\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),         # Third hidden layer with 64 neurons\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 32),          # Fourth hidden layer with 32 neurons\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1)            # Output layer\n",
        ")\n",
        "\n",
        "# Xavier initialization for weights\n",
        "for layer in model.children():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer (Adam with a lower learning rate)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop with increased epochs\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train_torch).squeeze()\n",
        "    # Compute loss\n",
        "    loss = loss_fn(y_pred, y_train_torch)\n",
        "\n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Clip gradients to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss during training\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train_torch).squeeze()\n",
        "    train_error = loss_fn(y_pred_train, y_train_torch)\n",
        "\n",
        "    y_pred_test = model(X_test_torch).squeeze()\n",
        "    test_error = loss_fn(y_pred_test, y_test_torch)\n",
        "\n",
        "print(f'\\nTraining error: {train_error.item()}')\n",
        "print(f'Testing error: {test_error.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnD1QqQWsgV6"
      },
      "source": [
        "# (Task 2 of Assignment 2): Subclassing nn.Module to build a neural network (the same network structure as Task 1) to fit the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAdaeMtSsgV7",
        "outputId": "3be668c1-3fbb-449f-c3ab-ce636ab8b058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Loss: 56189612032.0\n",
            "Epoch 50/500, Loss: 56181542912.0\n",
            "Epoch 100/500, Loss: 56123289600.0\n",
            "Epoch 150/500, Loss: 55922630656.0\n",
            "Epoch 200/500, Loss: 55436750848.0\n",
            "Epoch 250/500, Loss: 54467080192.0\n",
            "Epoch 300/500, Loss: 52804628480.0\n",
            "Epoch 350/500, Loss: 50219483136.0\n",
            "Epoch 400/500, Loss: 46475104256.0\n",
            "Epoch 450/500, Loss: 41399181312.0\n",
            "\n",
            "Training error: 34970099712.0\n",
            "Testing error: 34811768832.0\n"
          ]
        }
      ],
      "source": [
        "# Define the custom neural network by subclassing nn.Module\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)  # First hidden layer with 256 neurons\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)        # Second hidden layer with 128 neurons\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 64)         # Third hidden layer with 64 neurons\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(64, 32)          # Fourth hidden layer with 32 neurons\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(32, 1)  # Output layer\n",
        "\n",
        "        # Xavier initialization for weights\n",
        "        for layer in [self.fc1, self.fc2, self.fc3, self.fc4, self.output_layer]:\n",
        "            torch.nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Number of input features\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Convert the numpy arrays to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).squeeze()\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32).squeeze()\n",
        "\n",
        "# Instantiate the custom model\n",
        "model = CustomModel(input_dim)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer (Adam with a lower learning rate)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop with increased epochs\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train_torch).squeeze()\n",
        "    # Compute loss\n",
        "    loss = loss_fn(y_pred, y_train_torch)\n",
        "\n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Clip gradients to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss during training\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train_torch).squeeze()\n",
        "    train_error = loss_fn(y_pred_train, y_train_torch)\n",
        "\n",
        "    y_pred_test = model(X_test_torch).squeeze()\n",
        "    test_error = loss_fn(y_pred_test, y_test_torch)\n",
        "\n",
        "print(f'\\nTraining error: {train_error.item()}')\n",
        "print(f'Testing error: {test_error.item()}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}